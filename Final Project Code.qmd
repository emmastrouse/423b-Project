---
title: "423b Final Project: Predicting Economic Mobility with Social Capital in U.S. Colleges"
format: pdf
editor: visual
---

```{r}
library(dplyr)
library(sjPlot)
library(stringr)
library(reshape2)
library(viridis)
library(mgcv)
library(tidymodels)
library(stacks)
library(tidyverse)
library(xgboost)
library(ggpubr)
library(pcr)
library(factoextra)
library(pls)
library(yardstick)

#load both datasets
college_sc <- read_csv("social_capital_college.csv")
college_mobility <- read.csv("mrc_table1.csv")

#make college code identical/rename columns/align column class
college_code <- str_sub(college_sc$college, 1, str_length(college_sc$college)-2)
college_sc$college_code <- college_code
colnames(college_mobility)[1] = "college_code"
college_mobility$college_code <- as.character(college_mobility$college_code)


#join two data sets
college_mob_sc <- college_sc |>
  full_join(college_mobility, by="college_code")

#reorder columns
college_mob_sc <- college_mob_sc |>
  relocate(college_code) |> 
  mutate_if(is.integer, as.numeric)
```

# Descriptive Statistics

```{r}
library(stargazer)
descriptive_stats <- college_mob_sc |> 
  select(par_median, k_median, par_q1, par_top1pc, mr_kq5_pq1, ec_own_ses_college, ec_parent_ses_college, ec_high_own_ses_college, ec_high_parent_ses_college, exposure_own_ses_college, exposure_parent_ses_college, bias_own_ses_college, bias_parent_ses_college, bias_high_own_ses_college, bias_high_parent_ses_college, clustering_college, support_ratio_college, volunteering_rate_college) 

# Create the descriptive statistics table
stargazer(as.data.frame(descriptive_stats), type = "text", out = "summarystats.html", title = "Table 1. Descriptive Statistics")


```

## Correlation Matrix Heatmap (Predictors Only)

```{r}
d_cor <- college_mob_sc |> 
  select(ec_own_ses_college, ec_high_own_ses_college, exposure_own_ses_college, bias_own_ses_college, bias_high_own_ses_college, clustering_college, support_ratio_college, volunteering_rate_college, mr_kq5_pq1) |> 
    #college_mob_sc, -c(college_code, zip, college, county, college_name, name, czname, state)) |> 
  na.omit() |> 
  mutate(across(everything(), ~as.numeric(.)))
cor(d_cor)

cormat <- cor(d_cor)

long_cormat <- cormat |> 
  as.data.frame() |> 
  mutate(var1 = rownames(cormat)) |> 
  pivot_longer(-var1, names_to = 'var2', values_to = 'r')

ggplot(data = long_cormat, aes(x=var1, y=var2, fill=r)) + 
  geom_tile()
 get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }
  
upper_tri <- get_upper_tri(cormat)
upper_tri

reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}


cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix
library(reshape2)
library(viridis)
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Create a ggheatmap
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
  scale_fill_viridis(option = "B", discrete = FALSE)+
  theme_minimal()+ # minimal theme
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()+
  labs(x = 'Variable 2', y = 'Variable 1', title = 'Correlation Matrix Heatmap')
 
# Print the heatmap
print(ggheatmap)
```

## Correlation Matrix Heatmap (All Vars)

```{r}
d_cor <- college_mob_sc |> 
  select(-c(college_code, zip, college, county, college_name, name, czname, state))|>
  na.omit() |> 
  mutate(across(everything(), ~as.numeric(.)))
cor(d_cor)

cormat <- cor(d_cor)

long_cormat <- cormat |> 
  as.data.frame() |> 
  mutate(var1 = rownames(cormat)) |> 
  pivot_longer(-var1, names_to = 'var2', values_to = 'r')

ggplot(data = long_cormat, aes(x=var1, y=var2, fill=r)) + 
  geom_tile()
 get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }
  
upper_tri <- get_upper_tri(cormat)
upper_tri

reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}


cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix
library(reshape2)
library(viridis)
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Create a ggheatmap
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
  scale_fill_viridis(option = "B", discrete = FALSE)+
  theme_minimal()+ # minimal theme
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()+
  labs(x = 'Variable 2', y = 'Variable 1', title = 'Correlation Matrix Heatmap')
 
# Print the heatmap
print(ggheatmap)
```

Looks like we have some multicolinearity, how to deal?

## Split Data

```{r pressure, echo=FALSE}
set.seed(31337)


d_split <- college_mob_sc |> 
  na.omit() |> 
  #puts 80% of data in training set
  initial_split(prop=0.8)

dim(training(d_split))
dim(testing(d_split))


d_folds <- vfold_cv(training(d_split), v=10)
d_folds

```

## Simple OLS

```{r}

all_vars <- names(college_mob_sc)
exclude_vars <- c("college_code", "zip", "college", "county", "college_name", "name", "czname", "state", "mr_kq5_pq1", "ktop1pc_cond_parq1", "kq5_cond_parq1", "mr_ktop1_pq1", "trend_parq1", "trend_bottom40", "count", "ec_own_ses_se_college", "ec_parent_ses_se_college", "ec_high_own_ses_se_college", "ec_high_parent_ses_se_college")

predictor_vars <- all_vars[!(all_vars %in% exclude_vars)]


formula_str <- paste0("mr_kq5_pq1 ~ ", paste(predictor_vars, collapse = " + "))

simple_lm <- lm(formula_str, data = college_mob_sc)
summary(simple_lm)

predictions <- predict(simple_lm, testing(d_split))

#write function for rmse:
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
rmse(testing(d_split)$mr_kq5_pq1, predictions)
```

## GAM Workflow

```{r}

gam_spec <- gen_additive_mod(adjust_deg_free = tune()) |> 
  set_engine("mgcv") |> 
             #, path_values = penalty_grid) |> 
  set_mode("regression")


gam_workflow <- workflow() |> 
  add_variables(outcomes = c(mr_kq5_pq1), 
                predictors = predictor_vars) |> 
  add_model(gam_spec, formula = mr_kq5_pq1 ~ s(ec_own_ses_college) + s(ec_high_own_ses_college)+s(exposure_own_ses_college)+s(bias_own_ses_college)+s(bias_high_own_ses_college)+s(clustering_college)+s(support_ratio_college)+ s(volunteering_rate_college))
```

## Tune & Fit GAM

```{r}
tuned_GAM <- gam_workflow |> 
  tune_grid(d_folds,
            grid=20,
            metrics = NULL) #metric_set(rmse)

#gam_fit <- last_fit(tuned_GAM, data = d)
best_rmse <- select_best(tuned_GAM, metric='rmse')

final_gam <- gam_workflow |> 
  finalize_workflow(best_rmse)
final_gam


gam_fit_test <- last_fit(final_gam, d_split)
gam_fit_test$.metrics
```

This is really bad.

Now, let's try to create an ensemble

## Create Model Specs for Models to be included in Ensemble

```{r}
elastic_net <- linear_reg(penalty=tune(), 
                          mixture=tune()) |> 
  set_engine('glmnet')

knn_reg <- nearest_neighbor(neighbors=tune()) |> 
  set_mode('regression') |> 
  set_engine('kknn')

lasso_model <- linear_reg(penalty=tune(), 
                          mixture=1) |> 
  set_engine('glmnet')

random_forest <- boost_tree(mtry = tune(), trees = tune(), tree_depth = tune()) |> 
  set_engine('xgboost') |> 
  set_mode('regression')

```

```{r}
# 
# formula = mr_kq5_pq1 ~ s(ec_own_ses_college) + s(ec_high_own_ses_college)+s(exposure_own_ses_college)+s(bias_own_ses_college)+s(bias_high_own_ses_college)+s(clustering_college)+s(support_ratio_college)+ s(volunteering_rate_college)

exclude_vars <- c("college_code", "zip", "college", "county", "college_name", "name", "czname", "state", "ktop1pc_cond_parq1", "kq5_cond_parq1", "mr_ktop1_pq1", "trend_parq1", "trend_bottom40", "count", "ec_own_ses_se_college", "ec_parent_ses_se_college", "ec_high_own_ses_se_college", "ec_high_parent_ses_se_college")

d_recipe <- recipe(mr_kq5_pq1 ~ ., data=training(d_split)) |> 
  step_rm(exclude_vars) |> 
  step_normalize(all_numeric_predictors()) |> 
  step_dummy(all_nominal_predictors())
```

## Ensemble Workflow

```{r}
wf_set <- workflow_set(
  preproc = list(d_recipe),
  models = list(elastic_net=elastic_net, 
                knn=knn_reg, 
                lasso=lasso_model,
                forest = random_forest)
)

wf_set
```

```{r}
grid_ctrl <- control_grid(save_pred = TRUE,
                          parallel_over = "everything",
                          save_workflow = TRUE)

grid_results <- wf_set |> 
   workflow_map(seed = 1337,
      resamples = d_folds,
      grid = 20,
      control = grid_ctrl)

grid_results
```

```{r}
autoplot(grid_results, 
         rank_metric = "rmse", 
         metric = "rmse") +
  theme_bw()+
  scale_color_brewer(palette = "Paired")+
  labs(title = 'RMSE Across Models in Ensemble, Ranked Best to Worst')
```

```{r}
pisa_stack <- stacks() |> 
  add_candidates(grid_results)

# meta_m <- blend_predictions(pisa_stack, penalty=10^seq(-6, 0, length.out=10), mixture = 1)
# 
# meta_m <- blend_predictions(pisa_stack, mtry=10^seq(0, 10, length.out=10), mixture = 1)
# 
# autoplot(meta_m)


meta_m <- blend_predictions(pisa_stack)
meta_m
autoplot(meta_m)
```

Ensemble Retained 16 out of 61 possible members, penalty = .01, mixture = 1

```{r}
autoplot(meta_m, "weights") +
  geom_text(aes(x = weight + 0.01, label = model), hjust = 0) + 
  theme(legend.position = "none") +
  scale_fill_brewer(palette = 'Paired') +
  lims(x = c(-0.01, 0.8)) +
  theme_bw()+
  labs(title = paste("Model Weights in Metamodel (kNN, Elastic Net, Lasso, Forest) \npenalty =", meta_m$penalty))
```

```{r}
ensemble <- fit_members(meta_m)
reg_metrics <- metric_set(yardstick::rmse, yardstick::rsq)


#bind predictions from ensemble to test data outcome 
ensemble_test_pred <- predict(ensemble, new_data=testing(d_split)) |> 
  bind_cols(testing(d_split))


ensemble_test_pred |> 
  reg_metrics(mr_kq5_pq1, .pred)
#RMSE = 0.692, rsq = .700

ggplot(ensemble_test_pred, aes(x=mr_kq5_pq1, y=.pred)) +
  geom_point(color='cornflowerblue', alpha=0.9) +
  geom_abline(aes(slope=1, intercept=0), lty=2) +
  theme_bw()+
  labs(x = 'Economic Mobility', y = 'Predicted Economic Mobilitiy', title = 'Prediction Performance: Actual vs Ensemble-Predicted Economic Mobility\nAcross Colleges in Test Set')

```

## Inference: Investigate Relationships with Ensemble

Following what we did in lab 4.2, create plots of predicted mobility as a function of the value of predictors. In each case, be sure to use appropriate values of the variable you are investigating and hold all other variables at their mean value. Interpret each of these plots. What is the ensemble showing you about the relationships between each of these variables and happiness?

Predictors: ec_own_ses_college, ec_high_own_ses_college, exposure_own_ses_college, bias_own_ses_college, bias_high_own_ses_college, clustering_college, support_ratio_college, volunteering_rate_college

outcome: mr_kq5_pq1

#### 1. Economic Connectedness

```{r}
college_mob_sc <- college_mob_sc |> 
  mutate(type = 'data')

summary(college_mob_sc$ec_own_ses_college)

#check out the distribution of beer variable to choose reasonable range
ggplot(data = college_mob_sc, aes(x = ec_own_ses_college)) +
  geom_histogram()


pred_ec_own <- college_mob_sc |> 
  mutate(across(where(is.numeric), ~ mean(., na.rm = TRUE))) |> 
  mutate(ec_own_ses_college = seq(0.2162, 1.9018, length.out=2933)) |> 
  select(-mr_kq5_pq1) |> 
  mutate(type = 'model')

gam_fit <- fit(final_gam, data=college_mob_sc)

pred_ec_own <- bind_cols(pred_ec_own, predict(gam_fit, new_data=pred_ec_own)) |> 
  rename(mr_kq5_pq1 = .pred)



p1 <- bind_rows(college_mob_sc, pred_ec_own) |> 
    #filter out outliers on beer_per_capita to scale our plot reasonably
  # mutate(z_score_beer_per_capita = scale(beer_per_capita)) |>
  # filter(z_score_beer_per_capita <= 3) |>
  ggplot(aes(x=ec_own_ses_college, y=mr_kq5_pq1, size=type, color=type)) + 
  geom_point() + 
  scale_color_manual(values=c('data'='black', 
                              'model'='cornflowerblue')) +
  scale_size_manual(values=c('data'=2,
                             'model'=0.3)) +
  labs(x = 'Economic Connectedness', y = 'Economic Mobility', title = 'Economic Connectedness vs Mobility Rate')+
  theme_bw()

```

#### 2. Economic Connectedness (Parental)

```{r}


summary(college_mob_sc$ec_parent_ses_college)

#check out the distribution of beer variable to choose reasonable range
ggplot(data = college_mob_sc, aes(x = ec_parent_ses_college)) +
  geom_histogram()


pred_ec_parent <- college_mob_sc |> 
  mutate(across(where(is.numeric), ~ mean(., na.rm = TRUE))) |> 
  mutate(ec_parent_ses_college = seq(0.2730, 1.7315, length.out=2933)) |> 
  select(-mr_kq5_pq1) |> 
  mutate(type = 'model')

gam_fit <- fit(final_gam, data=college_mob_sc)

pred_ec_parent <- bind_cols(pred_ec_parent, predict(gam_fit, new_data=pred_ec_parent)) |> 
  rename(mr_kq5_pq1 = .pred)



p2 <- bind_rows(college_mob_sc, pred_ec_parent) |> 
    #filter out outliers on beer_per_capita to scale our plot reasonably
  # mutate(z_score_beer_per_capita = scale(beer_per_capita)) |>
  # filter(z_score_beer_per_capita <= 3) |>
  ggplot(aes(x=ec_parent_ses_college, y=mr_kq5_pq1, size=type, color=type)) + 
  geom_point() + 
  scale_color_manual(values=c('data'='black', 
                              'model'='cornflowerblue')) +
  scale_size_manual(values=c('data'=2,
                             'model'=0.3)) +
  labs(x = 'Parent Economic Connectedness', y = 'Economic Mobility', title = 'Economic Connectedness with Parental SES vs Mobility Rate')+
  theme_bw()
```

#### 3. High Economic Connectedness

```{r}
summary(college_mob_sc$ec_high_own_ses_college)

#check out the distribution of beer variable to choose reasonable range
ggplot(data = college_mob_sc, aes(x = ec_high_own_ses_college)) +
  geom_histogram()


pred_ec_high_own <- college_mob_sc |> 
  mutate(across(where(is.numeric), ~ mean(., na.rm = TRUE))) |> 
  mutate(ec_high_own_ses_college = seq(0.3355, 1.9347, length.out=2933)) |> 
  select(-mr_kq5_pq1) |> 
  mutate(type = 'model')
                 

pred_ec_high_own <- bind_cols(pred_ec_high_own, predict(gam_fit, new_data=pred_ec_high_own)) |> 
  rename(mr_kq5_pq1 = .pred)



p3 <- bind_rows(college_mob_sc, pred_ec_high_own) |> 
  ggplot(aes(x=ec_high_own_ses_college, y=mr_kq5_pq1, size=type, color=type)) + 
  geom_point() + 
  scale_color_manual(values=c('data'='black', 
                              'model'='cornflowerblue')) +
  scale_size_manual(values=c('data'=2,
                             'model'=0.3)) +
  labs(x = 'Economic Connectedness (High)', y = 'Economic Mobility', title = 'Economic Connectedness for High-SES Individuals vs Mobility Rate')+
  theme_bw()

```

#### 4. High Economic Connectedness (Parental)

```{r}
summary(college_mob_sc$ec_high_parent_ses_college)

#check out the distribution of beer variable to choose reasonable range
ggplot(data = college_mob_sc, aes(x = ec_high_parent_ses_college)) +
  geom_histogram()


pred_ec_high_parent <- college_mob_sc |> 
  mutate(across(where(is.numeric), ~ mean(., na.rm = TRUE))) |> 
  mutate(ec_high_parent_ses_college = seq(0.2793, 1.7421, length.out=2933)) |> 
  select(-mr_kq5_pq1) |> 
  mutate(type = 'model')
                 

pred_ec_high_parent <- bind_cols(pred_ec_high_parent, predict(gam_fit, new_data=pred_ec_high_parent)) |> 
  rename(mr_kq5_pq1 = .pred)



p4 <- bind_rows(college_mob_sc, pred_ec_high_parent) |> 
  ggplot(aes(x=ec_high_parent_ses_college, y=mr_kq5_pq1, size=type, color=type)) + 
  geom_point() + 
  scale_color_manual(values=c('data'='black', 
                              'model'='cornflowerblue')) +
  scale_size_manual(values=c('data'=2,
                             'model'=0.3)) +
  labs(x = 'Parent Economic Connectedness (High)', y = 'Economic Mobility', title = 'Economic connectedness for high-parental-SES individuals vs Mobility Rate')+
  theme_bw()

```

#### 5. Exposure

```{r}
summary(college_mob_sc$exposure_own_ses_college)

#check out the distribution of beer variable to choose reasonable range
ggplot(data = college_mob_sc, aes(x = exposure_own_ses_college)) +
  geom_histogram()


pred_exposure_own <- college_mob_sc |> 
  mutate(across(where(is.numeric), ~ mean(., na.rm = TRUE))) |> 
  mutate(exposure_own_ses_college = seq(0.265, 1.9, length.out=2933)) |> 
  select(-mr_kq5_pq1) |> 
  mutate(type = 'model')


pred_exposure_own <- bind_cols(pred_exposure_own, predict(gam_fit, new_data=pred_exposure_own)) |> 
  rename(mr_kq5_pq1 = .pred)


p5 <- bind_rows(college_mob_sc, pred_exposure_own) |> 
  ggplot(aes(x=exposure_own_ses_college, y=mr_kq5_pq1, size=type, color=type)) + 
  geom_point() + 
  scale_color_manual(values=c('data'='black', 
                              'model'='cornflowerblue')) +
  scale_size_manual(values=c('data'=2,
                             'model'=0.3)) +
  labs(x = 'Exposure', y = 'Economic Mobility', title = 'Mean exposure to high-SES individuals for low-SES individuals\nvs Mobility Rate')+
  theme_bw()

```

#### 6. Exposure (Parental)

```{r}
summary(college_mob_sc$exposure_parent_ses_college)

#check out the distribution of beer variable to choose reasonable range
ggplot(data = college_mob_sc, aes(x = exposure_parent_ses_college)) +
  geom_histogram()


pred_exposure_parent <- college_mob_sc |> 
  mutate(across(where(is.numeric), ~ mean(., na.rm = TRUE))) |> 
  mutate(exposure_parent_ses_college = seq(0.2906, 1.7188, length.out=2933)) |> 
  select(-mr_kq5_pq1) |> 
  mutate(type = 'model')


pred_exposure_parent <- bind_cols(pred_exposure_parent, predict(gam_fit, new_data=pred_exposure_parent)) |> 
  rename(mr_kq5_pq1 = .pred)


p6 <- bind_rows(college_mob_sc, pred_exposure_parent) |> 
  ggplot(aes(x=exposure_parent_ses_college, y=mr_kq5_pq1, size=type, color=type)) + 
  geom_point() + 
  scale_color_manual(values=c('data'='black', 
                              'model'='cornflowerblue')) +
  scale_size_manual(values=c('data'=2,
                             'model'=0.3)) +
  labs(x = 'Exposure', y = 'Economic Mobility', title = 'Mean exposure to high-parental-SES individuals by college for low- parental-SES individuals\nvs Mobility Rate')+
  theme_bw()
```

#### 7. Bias

```{r}
summary(college_mob_sc$bias_own_ses_college)

#check out the distribution of beer variable to choose reasonable range
ggplot(data = college_mob_sc, aes(x = bias_own_ses_college)) +
  geom_histogram()


pred_bias_own <- college_mob_sc |> 
  mutate(across(where(is.numeric), ~ mean(., na.rm = TRUE))) |> 
  mutate(bias_own_ses_college = seq(-.1562, .3822, length.out=2933)) |> 
  select(-mr_kq5_pq1) |> 
  mutate(type = 'model')

                 

pred_bias_own <- bind_cols(pred_bias_own, predict(gam_fit, new_data=pred_bias_own)) |> 
  rename(mr_kq5_pq1 = .pred)



p7 <- bind_rows(college_mob_sc, pred_bias_own) |> 
  ggplot(aes(x=bias_own_ses_college, y=mr_kq5_pq1, size=type, color=type)) + 
  geom_point() + 
  scale_color_manual(values=c('data'='black', 
                              'model'='cornflowerblue')) +
  scale_size_manual(values=c('data'=2,
                             'model'=0.3)) +
  labs(x = 'Friending Bias', y = 'Economic Mobility', title = 'Friending Bias vs Mobility Rate')+
  theme_bw()

```

#### 8. Bias (Parental)

```{r}
summary(college_mob_sc$bias_parent_ses_college)

#check out the distribution of beer variable to choose reasonable range
ggplot(data = college_mob_sc, aes(x = bias_parent_ses_college)) +
  geom_histogram()


pred_bias_parent <- college_mob_sc |> 
  mutate(across(where(is.numeric), ~ mean(., na.rm = TRUE))) |> 
  mutate(bias_parent_ses_college = seq(-.1650, .2440, length.out=2933)) |> 
  select(-mr_kq5_pq1) |> 
  mutate(type = 'model')

                 

pred_bias_parent <- bind_cols(pred_bias_parent, predict(gam_fit, new_data=pred_bias_parent)) |> 
  rename(mr_kq5_pq1 = .pred)



p8 <- bind_rows(college_mob_sc, pred_bias_parent) |> 
  ggplot(aes(x=bias_parent_ses_college, y=mr_kq5_pq1, size=type, color=type)) + 
  geom_point() + 
  scale_color_manual(values=c('data'='black', 
                              'model'='cornflowerblue')) +
  scale_size_manual(values=c('data'=2,
                             'model'=0.3)) +
  labs(x = 'Friending Bias with Parental SES', y = 'Economic Mobility', title = 'Friending Bias using Parental SES vs Mobility Rate')+
  theme_bw()
```

#### 9. Bias (High)

```{r}
summary(college_mob_sc$bias_high_own_ses_college)

#check out the distribution of beer variable to choose reasonable range
ggplot(data = college_mob_sc, aes(x = bias_high_own_ses_college)) +
  geom_histogram()


pred_bias_high_own <- college_mob_sc |> 
  mutate(across(where(is.numeric), ~ mean(., na.rm = TRUE))) |> 
  mutate(bias_high_own_ses_college = seq(-.834, .0225, length.out=2933)) |> 
  select(-mr_kq5_pq1) |> 
  mutate(type = 'model')


pred_bias_high_own <- bind_cols(pred_bias_high_own, predict(gam_fit, new_data=pred_bias_high_own)) |> 
  rename(mr_kq5_pq1 = .pred)



p9 <- bind_rows(college_mob_sc, pred_bias_high_own) |> 
    #filter out outliers on beer_per_capita to scale our plot reasonably
  # mutate(z_score_beer_per_capita = scale(beer_per_capita)) |>
  # filter(z_score_beer_per_capita <= 3) |>
  ggplot(aes(x=bias_high_own_ses_college, y=mr_kq5_pq1, size=type, color=type)) + 
  geom_point() + 
  scale_color_manual(values=c('data'='black', 
                              'model'='cornflowerblue')) +
  scale_size_manual(values=c('data'=2,
                             'model'=0.3)) +
  labs(x = 'Friending Bias (High)', y = 'Economic Mobility', title = 'Friending Bias for High-SES Individuals vs Mobility Rate')+
  theme_bw()
```

#### 10. Bias (Parental; High)

```{r}
summary(college_mob_sc$bias_high_parent_ses_college)

# ggplot(data = college_mob_sc, aes(x = bias_high_parent_ses_college)) +
#   geom_histogram()


pred_bias_high_parent <- college_mob_sc |> 
  mutate(across(where(is.numeric), ~ mean(., na.rm = TRUE))) |> 
  mutate(bias_high_parent_ses_college = seq(-.3357, .0973, length.out=2933)) |> 
  select(-mr_kq5_pq1) |> 
  mutate(type = 'model')


pred_bias_high_parent <- bind_cols(pred_bias_high_parent, predict(gam_fit, new_data=pred_bias_high_parent)) |> 
  rename(mr_kq5_pq1 = .pred)



p10 <- bind_rows(college_mob_sc, pred_bias_high_parent) |> 
  ggplot(aes(x=bias_high_parent_ses_college, y=mr_kq5_pq1, size=type, color=type)) + 
  geom_point() + 
  scale_color_manual(values=c('data'='black', 
                              'model'='cornflowerblue')) +
  scale_size_manual(values=c('data'=2,
                             'model'=0.3)) +
  labs(x = 'Friending Bias (High Parental SES)', y = 'Economic Mobility', title = 'Friending Bias for High-Parental-SES Individuals vs Mobility Rate')+
  theme_bw()
```

#### 11. Clustering

```{r}
summary(college_mob_sc$clustering_college)

#check out the distribution of beer variable to choose reasonable range
ggplot(data = college_mob_sc, aes(x = clustering_college)) +
  geom_histogram()


pred_clustering <- college_mob_sc |> 
  mutate(across(where(is.numeric), ~ mean(., na.rm = TRUE))) |> 
  mutate(clustering_college = seq(.0989, .8225, length.out=2933)) |> 
  select(-mr_kq5_pq1) |> 
  mutate(type = 'model')


pred_clustering <- bind_cols(pred_clustering, predict(gam_fit, new_data=pred_clustering)) |> 
  rename(mr_kq5_pq1 = .pred)



p11 <- bind_rows(college_mob_sc, pred_clustering) |> 
  ggplot(aes(x=clustering_college, y=mr_kq5_pq1, size=type, color=type)) + 
  geom_point() + 
  scale_color_manual(values=c('data'='black', 
                              'model'='cornflowerblue')) +
  scale_size_manual(values=c('data'=2,
                             'model'=0.3)) +
  labs(x = 'Clustering', y = 'Economic Mobility', title = 'Clustering vs Mobility Rate')+
  theme_bw()
```

#### 12. Support Ratio

```{r}
summary(college_mob_sc$support_ratio_college)

#check out the distribution of beer variable to choose reasonable range
ggplot(data = college_mob_sc, aes(x = support_ratio_college)) +
  geom_histogram()


pred_support_ratio <- college_mob_sc |> 
  mutate(across(where(is.numeric), ~ mean(., na.rm = TRUE))) |> 
  mutate(support_ratio_college = seq(.5166, 1.0, length.out=2933)) |> 
  select(-mr_kq5_pq1) |> 
  mutate(type = 'model')


pred_support_ratio <- bind_cols(pred_support_ratio, predict(gam_fit, new_data=pred_support_ratio)) |> 
  rename(mr_kq5_pq1 = .pred)



p12 <- bind_rows(college_mob_sc, pred_support_ratio) |> 
  ggplot(aes(x=support_ratio_college, y=mr_kq5_pq1, size=type, color=type)) + 
  geom_point() + 
  scale_color_manual(values=c('data'='black', 
                              'model'='cornflowerblue')) +
  scale_size_manual(values=c('data'=2,
                             'model'=0.3)) +
  labs(x = 'Economic Segregation (Own)', y = 'Economic Mobility', title = 'Support Ratio vs Mobility Rate')+
  theme_bw()
```

#### 13. Volunteering Rate

```{r}
summary(college_mob_sc$volunteering_rate_college)

#check out the distribution of beer variable to choose reasonable range
ggplot(data = college_mob_sc, aes(x = volunteering_rate_college)) +
  geom_histogram()


pred_volunteering_rate <- college_mob_sc |> 
  mutate(across(where(is.numeric), ~ mean(., na.rm = TRUE))) |> 
  mutate(volunteering_rate_college = seq(.0014, .3471, length.out=2933)) |> 
  select(-mr_kq5_pq1) |> 
  mutate(type = 'model')


pred_volunteering_rate <- bind_cols(pred_volunteering_rate, predict(gam_fit, new_data=pred_volunteering_rate)) |> 
  rename(mr_kq5_pq1 = .pred)



p13 <- bind_rows(college_mob_sc, pred_volunteering_rate) |> 
  ggplot(aes(x=volunteering_rate_college, y=mr_kq5_pq1, size=type, color=type)) + 
  geom_point() + 
  scale_color_manual(values=c('data'='black', 
                              'model'='cornflowerblue')) +
  scale_size_manual(values=c('data'=2,
                             'model'=0.3)) +
  labs(x = 'Volunteering Rate', y = 'Economic Mobility', title = 'Volunteering Rate vs Mobility Rate')+
  theme_bw()
```

Print out plots here to save:

```{r}
p1
p2
p3
p4
p5
p6
p7
p8
p9
p10
p11
p12
p13

```

# PCA:

try out PCA to deal with the colinearity of predictors in our simple OLS

```{r}

college_pca <- training(d_split) |> 
  select(ec_own_ses_college, ec_parent_ses_college, ec_high_own_ses_college,ec_high_parent_ses_college, exposure_own_ses_college, exposure_parent_ses_college, bias_own_ses_college, bias_parent_ses_college, bias_high_own_ses_college, bias_high_parent_ses_college, clustering_college, support_ratio_college, volunteering_rate_college, mr_kq5_pq1) |> 
  na.omit() |> 
  mutate(across(everything(), ~as.numeric(.)))
set.seed(100)
pcr_college <- pcr(mr_kq5_pq1~., data = college_pca, scale = TRUE, validation = "CV")
summary(pcr_college)
# Plot the root mean squared error
validationplot(pcr_college)
# Plot the cross validation MSE
validationplot(pcr_college, val.type="MSEP")
# Plot the R2
validationplot(pcr_college, val.type = "R2")
##plot the predicted vs measured values
predplot(pcr_college)
#plot regression coeff
coefplot(pcr_college)
data_split <- initial_split(college_pca, prop = 0.8)
# extract the training and test sets from the split object
train_data <- training(data_split)
test_data <- testing(data_split)
y_test <- test_data[,9]
pcr_model <- pcr(mr_kq5_pq1~., data = train_data,scale =TRUE, validation = "CV")
pcr_pred <- predict(pcr_model, test_data, ncomp = 3)
mean((pcr_pred - y_test)^2)
```

There do not seem to be a few PCs explaining a lot of the variance; performance does not 'level out' but continues to improve with additional PCs

```{r}
pca_2 <- prcomp( ~ ec_own_ses_college + ec_parent_ses_college + ec_high_own_ses_college + ec_high_parent_ses_college + exposure_own_ses_college + exposure_parent_ses_college + bias_own_ses_college + bias_parent_ses_college + bias_high_own_ses_college + bias_high_parent_ses_college + clustering_college + support_ratio_college + volunteering_rate_college, data=college_pca, center=TRUE, scale.=TRUE)

pca_2
plot(pca_2)
summary(pca_2)


#bind principal components to OG dataset
d_pca <- bind_cols(training(d_split), pca_2$x) |> 
  na.omit()

#plot the first 2 PCs against each other
ggplot(d_pca, aes(x=PC1, y=PC2)) +
  geom_point() +
  labs(title='Plotting first two PCs') +
  theme_bw()

ggplot(d_pca, aes(x=PC1, y=mr_kq5_pq1)) +
  geom_point() +
  labs(title='Plotting Mobility vs first PC') +
  theme_bw()

ggplot(d_pca, aes(x=PC2, y=mr_kq5_pq1)) +
  geom_point() +
  labs(title='Plotting Mobility vs second PC') +
  theme_bw()

ggplot(d_pca, aes(x=PC3, y=mr_kq5_pq1)) +
  geom_point() +
  labs(title='Plotting Mobility vs second PC') +
  theme_bw()

ggplot(d_pca, aes(x=PC4, y=mr_kq5_pq1)) +
  geom_point() +
  labs(title='Plotting Mobility vs second PC') +
  theme_bw()


```

Explore PCA in an OLS?

```{r}

all_vars_pca <- names(d_pca)
exclude_vars_pca <- c("college_code", "zip", "college", "county", "college_name", "name", "czname", "state", "mr_kq5_pq1", "ktop1pc_cond_parq1", "kq5_cond_parq1", "mr_ktop1_pq1", "trend_parq1", "trend_bottom40", "count", "ec_own_ses_se_college", "ec_parent_ses_se_college", "ec_high_own_ses_se_college", "ec_high_parent_ses_se_college", 'ec_own_ses_college', 'ec_parent_ses_college',  'ec_high_own_ses_college',  'ec_high_parent_ses_college',  'exposure_own_ses_college',  'exposure_parent_ses_college', 'bias_own_ses_college',  'bias_parent_ses_college',  'bias_high_own_ses_college', 'bias_high_parent_ses_college','clustering_college', 'support_ratio_college',  'volunteering_rate_college')

predictor_vars_pca <- all_vars_pca[!(all_vars_pca %in% exclude_vars_pca)]


formula_str_pca <- paste0("mr_kq5_pq1 ~ ", paste(predictor_vars_pca, collapse = " + "))

simple_lm_pca <- lm(formula_str_pca, data = d_pca)
summary(simple_lm_pca)

predictions_pca <- predict(simple_lm_pca, d_pca)
rmse(d_pca$mr_kq5_pq1, predictions_pca)
```

Performance does not improve much from our simple OLS, even inclidng all 13 PCs in the model. Therefore, we decide to stick to interpreting our simple OLS coefficients

# Inference: Investigate Relationships with Simple OLS

```{r}
summary_model <- summary(simple_lm)

# Extract the coefficient estimates
coef_table <- coef(summary_model)

# View the table of coefficient estimates
coef_table



table <- tab_model(simple_lm,
                   show.est = TRUE, show.se = TRUE)

table
```
