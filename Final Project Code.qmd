---
title: "423b Final Project: Predicting Economic Mobility with Social Capital in U.S. Colleges"
format: pdf
editor: visual
---

```{r}
library(dplyr)
library(stringr)
library(reshape2)
library(viridis)
library(mgcv)
library(tidymodels)
library(stacks)
library(tidyverse)
library(xgboost)

#load both datasets
college_sc <- read_csv("social_capital_college.csv")
college_mobility <- read.csv("mrc_table1.csv")

#make college code identical/rename columns/align column class
college_code <- str_sub(college_sc$college, 1, str_length(college_sc$college)-2)
college_sc$college_code <- college_code
colnames(college_mobility)[1] = "college_code"
college_mobility$college_code <- as.character(college_mobility$college_code)


#join two data sets
college_mob_sc <- college_sc |>
  full_join(college_mobility, by="college_code")

#reorder columns
college_mob_sc <- college_mob_sc |>
  relocate(college_code) 
```

## Correlation Matrix Heatmap (Predictors Only)

```{r}
d_cor <- college_mob_sc |> 
  select(ec_own_ses_college, ec_high_own_ses_college, exposure_own_ses_college, bias_own_ses_college, bias_high_own_ses_college, clustering_college, support_ratio_college, volunteering_rate_college, mr_kq5_pq1) |> 
    #college_mob_sc, -c(college_code, zip, college, county, college_name, name, czname, state)) |> 
  na.omit() |> 
  mutate(across(everything(), ~as.numeric(.)))
cor(d_cor)

cormat <- cor(d_cor)

long_cormat <- cormat |> 
  as.data.frame() |> 
  mutate(var1 = rownames(cormat)) |> 
  pivot_longer(-var1, names_to = 'var2', values_to = 'r')

ggplot(data = long_cormat, aes(x=var1, y=var2, fill=r)) + 
  geom_tile()
 get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }
  
upper_tri <- get_upper_tri(cormat)
upper_tri

reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}


cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix
library(reshape2)
library(viridis)
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Create a ggheatmap
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
  scale_fill_viridis(option = "B", discrete = FALSE)+
  theme_minimal()+ # minimal theme
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()+
  labs(x = 'Variable 2', y = 'Variable 1', title = 'Correlation Matrix Heatmap')
 
# Print the heatmap
print(ggheatmap)
```

## Correlation Matrix Heatmap (All Vars)

```{r}
d_cor <- college_mob_sc |> 
  select(-c(college_code, zip, college, county, college_name, name, czname, state))|>
  na.omit() |> 
  mutate(across(everything(), ~as.numeric(.)))
cor(d_cor)

cormat <- cor(d_cor)

long_cormat <- cormat |> 
  as.data.frame() |> 
  mutate(var1 = rownames(cormat)) |> 
  pivot_longer(-var1, names_to = 'var2', values_to = 'r')

ggplot(data = long_cormat, aes(x=var1, y=var2, fill=r)) + 
  geom_tile()
 get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }
  
upper_tri <- get_upper_tri(cormat)
upper_tri

reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}


cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix
library(reshape2)
library(viridis)
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Create a ggheatmap
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
  scale_fill_viridis(option = "B", discrete = FALSE)+
  theme_minimal()+ # minimal theme
 theme(axis.text.x = element_text(angle = 45, vjust = 1, 
    size = 12, hjust = 1))+
 coord_fixed()+
  labs(x = 'Variable 2', y = 'Variable 1', title = 'Correlation Matrix Heatmap')
 
# Print the heatmap
print(ggheatmap)
```

## Looks like we have some multicolinearity, how to deal?

```{r}
corr_matrix <- cor(d_cor)

# Find the pairs of variables with high correlation
high_corr <- which(abs(corr_matrix) > 0.6 & corr_matrix != 1, arr.ind = TRUE)

if (length(high_corr) > 0) {
  #cat("The following pairs of variables have high correlation (> 0.6):\n")
  for (i in 1:nrow(high_corr)) {
    var1 <- names(corr_matrix)[high_corr[i,1]]
    var2 <- names(corr_matrix)[high_corr[i,2]]
    #cat(var1, "and", var2, "\n")
    # or
    print(paste(var1, "and", var2))
    #cat(paste(var1, "and", var2, "\n"))
  }
} else {
  cat("No pairs of variables have high correlation (> 0.6).\n")
}


```

## Split Data

```{r pressure, echo=FALSE}
set.seed(31337)


d_split <- college_mob_sc |> 
  na.omit() |> 
  #puts 80% of data in training set
  initial_split(prop=0.8)

dim(training(d_split))
dim(testing(d_split))


d_folds <- vfold_cv(training(d_split), v=10)
d_folds

```

## GAM Workflow

```{r}


gam_spec <- gen_additive_mod(adjust_deg_free = tune()) |> 
  set_engine("mgcv") |> 
             #, path_values = penalty_grid) |> 
  set_mode("regression")

all_vars <- names(college_mob_sc)
exclude_vars <- c("college_code", "zip", "college", "county", "college_name", "name", "czname", "state", "mr_kq5_pq1", "ktop1pc_cond_parq1", "kq5_cond_parq1", "mr_ktop1_pq1", "trend_parq1", "trend_bottom40", "count", "ec_own_ses_se_college", "ec_parent_ses_se_college", "ec_high_own_ses_se_college", "ec_high_parent_ses_se_college")

predictor_vars <- all_vars[!(all_vars %in% exclude_vars)]

gam_workflow <- workflow() |> 
  add_variables(outcomes = c(mr_kq5_pq1), 
                predictors = predictor_vars) |> 
  add_model(gam_spec, formula = mr_kq5_pq1 ~ s(ec_own_ses_college) + s(ec_high_own_ses_college)+s(exposure_own_ses_college)+s(bias_own_ses_college)+s(bias_high_own_ses_college)+s(clustering_college)+s(support_ratio_college)+ s(volunteering_rate_college))
```

## Tune & Fit GAM

```{r}
tuned_GAM <- gam_workflow |> 
  tune_grid(d_folds,
            grid=20,
            metrics = NULL) #metric_set(rmse)

#gam_fit <- last_fit(tuned_GAM, data = d)
best_rmse <- select_best(tuned_GAM, metric='rmse')

final_gam <- gam_workflow |> 
  finalize_workflow(best_rmse)
final_gam


gam_fit_test <- last_fit(final_gam, d_split)
gam_fit_test$.metrics
```

#this is really bad

## Create Model Specs for Models to be included in Ensemble

```{r}
elastic_net <- linear_reg(penalty=tune(), 
                          mixture=tune()) |> 
  set_engine('glmnet')

knn_reg <- nearest_neighbor(neighbors=tune()) |> 
  set_mode('regression') |> 
  set_engine('kknn')

lasso_model <- linear_reg(penalty=tune(), 
                          mixture=1) |> 
  set_engine('glmnet')

random_forest <- boost_tree(mtry = tune(), trees = tune(), tree_depth = tune()) |> 
  set_engine('xgboost') |> 
  set_mode('regression')

```

```{r}
# 
# formula = mr_kq5_pq1 ~ s(ec_own_ses_college) + s(ec_high_own_ses_college)+s(exposure_own_ses_college)+s(bias_own_ses_college)+s(bias_high_own_ses_college)+s(clustering_college)+s(support_ratio_college)+ s(volunteering_rate_college)

exclude_vars <- c("college_code", "zip", "college", "county", "college_name", "name", "czname", "state", "ktop1pc_cond_parq1", "kq5_cond_parq1", "mr_ktop1_pq1", "trend_parq1", "trend_bottom40", "count", "ec_own_ses_se_college", "ec_parent_ses_se_college", "ec_high_own_ses_se_college", "ec_high_parent_ses_se_college")

d_recipe <- recipe(mr_kq5_pq1 ~ ., data=training(d_split)) |> 
  step_rm(exclude_vars) |> 
  step_normalize(all_numeric_predictors()) |> 
  step_dummy(all_nominal_predictors())
```

## Ensemble Workflow

```{r}
wf_set <- workflow_set(
  preproc = list(d_recipe),
  models = list(elastic_net=elastic_net, 
                knn=knn_reg, 
                lasso=lasso_model,
                forest = random_forest)
)

wf_set
```

```{r}

gam_wf_set <- as_workflow_set(gam = gam_workflow)
wf_set2 <- bind_rows(wf_set, gam_wf_set)
wf_set2
#this didn't work (couldn't add gam)
#for now i use original wf_set but feel free to try with wf_set2 if it works for you
```

```{r}
grid_ctrl <- control_grid(save_pred = TRUE,
                          parallel_over = "everything",
                          save_workflow = TRUE)

grid_results <- wf_set |> 
   workflow_map(seed = 1337,
      resamples = d_folds,
      grid = 20,
      control = grid_ctrl)

grid_results
```

```{r}
autoplot(grid_results, 
         rank_metric = "rmse", 
         metric = "rmse") +
  theme_bw()+
  scale_color_brewer(palette = "Paired")
```

```{r}
pisa_stack <- stacks() |> 
  add_candidates(grid_results)


meta_m <- blend_predictions(pisa_stack, penalty=10^seq(-6, 0, length.out=10), mixture = 1)

autoplot(meta_m)
```

```{r}

autoplot(meta_m, "weights") +
  geom_text(aes(x = weight + 0.01, label = model), hjust = 0) + 
  theme(legend.position = "none") +
  scale_fill_brewer(palette = 'Paired') +
  lims(x = c(-0.01, 0.8)) +
  theme_bw()+
  labs(title = paste("Model Weights in Metamodel\npenalty =", meta_m$penalty))
```

```{r}
ensemble <- fit_members(meta_m)

reg_metrics <- metric_set(rmse, rsq)

#bind predictions from ensemble to test data outcome 
ensemble_test_pred <- predict(ensemble, new_data=testing(d_split)) |> 
  bind_cols(testing(d_split))


ensemble_test_pred |> 
  reg_metrics(mr_kq5_pq1, .pred)
#RMSE = 0.692, rsq = .700

ggplot(ensemble_test_pred, aes(x=mr_kq5_pq1, y=.pred)) +
  geom_point(color='cornflowerblue', alpha=0.9) +
  geom_abline(aes(slope=1, intercept=0), lty=2) +
  theme_bw()+
  labs(x = 'Economic Mobility', y = 'Predicted Economic Mobilitiy', title = 'Actual vs Predicted Economic Mobility Across Colleges in Test Set')

```

## Investigate Relationships

Following what we did in lab 4.2, create plots of predicted mobility as a function of the value of predictors. In each case, be sure to use appropriate values of the variable you are investigating and hold all other variables at their mean value. Interpret each of these plots. What is the ensemble showing you about the relationships between each of these variables and happiness?

Predictors: ec_own_ses_college, ec_high_own_ses_college, exposure_own_ses_college, bias_own_ses_college, bias_high_own_ses_college, clustering_college, support_ratio_college, volunteering_rate_college

outcome: mr_kq5_pq1

```{r}
college_mob_sc <- college_mob_sc |> 
  mutate(type = 'data')

summary(college_mob_sc$ec_own_ses_college)

#check out the distribution of beer variable to choose reasonable range
ggplot(data = college_mob_sc, aes(x = ec_own_ses_college)) +
  geom_histogram()

mean(college_mob_sc$ec_high_own_ses_college, na.rm = TRUE)

pred_ec_own <- data.frame(ec_own_ses_college=seq(0.2162, 1.9018, length.out=2933),
                   ec_high_own_ses_college=mean(college_mob_sc$ec_high_own_ses_college, na.rm = T),
                   ec_parent_ses_college = mean(college_mob_sc$ec_parent_ses_college, na.rm = T),
                   ec_high_parent_ses_college = mean(college_mob_sc$ec_high_parent_ses_college, na.rm = T),
                   exposure_own_ses_college = mean(college_mob_sc$exposure_own_ses_college, na.rm = T),
                   exposure_parent_ses_college = mean(college_mob_sc$exposure_parent_ses_college, na.rm = T),
                   bias_own_ses_college = mean(college_mob_sc$bias_own_ses_college, na.rm = T),
                   bias_parent_ses_college = mean(college_mob_sc$bias_parent_ses_college, na.rm = T),
                   bias_high_own_ses_college = mean(college_mob_sc$bias_high_own_ses_college, na.rm = T),
                   bias_high_parent_ses_college = mean(college_mob_sc$bias_high_own_ses_college, na.rm = T),
                   clustering_colege = mean(college_mob_sc$clustering_college, na.rm. = T),
                   support_ratio_college = mean(college_mob_sc$support_ratio_college, na.rm = T),
                   volunteering_college = mean(college_mob_sc$volunteering_rate_college, na.rm = T),
                   mean_students_per_cohort = mean(college_mob_sc$mean_students_per_cohort, na.rm = T),
                   par_median = mean(college_mob_sc$par_median, na.rm = T),
                   k_median = mean(college_mob_sc$k_median, na.rm = T),
                   par_q1 = mean(college_mob_sc$par_q1, na.rm = T),
                   par_top1c = mean(college_mob_sc$par_top1c, na.rm = T),
                   type = 'model')

# length(exclude_vars)
# dim(college_mob_sc)
                 

gam_fit <- fit(final_gam, data=college_mob_sc)

pred_ec_own <- bind_cols(pred_ec_own, predict(gam_fit, new_data=pred_ec_own)) |> 
  rename(mr_kq5_pq1 = .pred)



bind_rows(college_mob_sc, pred_ec_own) |> 
    #filter out outliers on beer_per_capita to scale our plot reasonably
  # mutate(z_score_beer_per_capita = scale(beer_per_capita)) |>
  # filter(z_score_beer_per_capita <= 3) |>
  ggplot(aes(x=ec_own_ses_college, y=mr_kq5_pq1, size=type, color=type)) + 
  geom_point() + 
  scale_color_manual(values=c('data'='black', 
                              'model'='cornflowerblue')) +
  scale_size_manual(values=c('data'=2,
                             'model'=0.3)) +
  labs(x = 'Economic Segregation (Own)', y = 'Economic Mobility', title = 'Economic Segregation vs Mobility Rate')+
  theme_bw()

```

It appears the happiness increases moderately as beer per capita increases

```{r}
range(d$wine_per_capita)
#check out the distribution of wine variable to choose reasonable range
ggplot(data = d, aes(x = wine_per_capita)) +
  geom_histogram()


pred_wine <- data.frame(wine_per_capita=seq(1, 370, length.out=115),
                   population=mean(d$population),
                   infant_mortality_per_thousand = mean(d$infant_mortality_per_thousand),
                   beer_per_capita = mean(d$wine_per_capita),
                   gdp = mean(d$gdp),
                   area_sq_mi = mean(d$area_sq_mi),
                   spirits_per_capita = mean(d$spirits_per_capita),
                   pop_density = mean(d$pop_density),
                   type='model')


pred_wine <- bind_cols(pred_wine, predict(gam_fit, new_data=pred_wine)) |> 
  rename(happiness = .pred)


bind_rows(d, pred_wine) |> 
  #filter out outliers on wine_per_capita to scale our plot reasonably
  mutate(z_score_wine_per_capita = scale(wine_per_capita)) |> 
  filter(z_score_wine_per_capita <= 3) |> 
  ggplot(aes(x=wine_per_capita, y=happiness, size=type, color=type)) + 
  geom_point() + 
  scale_color_manual(values=c('data'='black', 
                              'model'='cornflowerblue')) +
  scale_size_manual(values=c('data'=2,
                             'model'=0.3)) +
  labs(x = 'Wine Per Capita', y = 'Happiness', title = 'Wine Per Capita vs Happiness\nGAM vs Data')+
  theme_bw()+
  #scale so we can see majority of data points
  scale_x_continuous(limits = c(0,280))

```
